JOB=1483827 ARRAY=1 HOST=node17
Fri Jan 23 19:19:34 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     On  |   00000000:89:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       0MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/usr/prakt/w0012/miniconda3/envs/tridi/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
2026-01-23 19:21:46 - INFO - tridi.utils.exp - Initializing wandb...
2026-01-23 19:21:46 - INFO - tridi.utils.exp - Trying to initialize WANDB try 0
wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
wandb: Currently logged in as: c-yang (yaoweiwang-technical-university-of-munich) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run h4maxurv
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /usr/prakt/w0012/HH_gen/experiments/001_chi3d_aug/wandb/run-20260123_192146-h4maxurv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 001_chi3d_aug
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yaoweiwang-technical-university-of-munich/hhgen
wandb: üöÄ View run at https://wandb.ai/yaoweiwang-technical-university-of-munich/hhgen/runs/h4maxurv
2026-01-23 19:21:53 - INFO - tridi.utils.exp - Trying to initialize WANDB try 1
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
2026-01-23 19:21:54 - INFO - tridi.utils.exp - Initialized WANDB
2026-01-23 19:21:54 - INFO - tridi.utils.training - Seeding node 0 with seed 42
2026-01-23 19:21:56 - INFO - tridi.model - Created model: transformer_unidiffuser_3 with 14.9M trainable params (14.9M total).
2026-01-23 19:21:57 - INFO - tridi.utils.training - lr = 0.00015 (absolute learning rate)
2026-01-23 19:21:58 - INFO - tridi.utils.training - Loading checkpoint from /usr/prakt/w0012/HH_gen/experiments/001_chi3d_aug/checkpoints/checkpoint-step-0015000.pth
2026-01-23 19:21:59 - INFO - tridi.utils.training - Loaded model checkpoint from /usr/prakt/w0012/HH_gen/experiments/001_chi3d_aug/checkpoints/checkpoint-step-0015000.pth
2026-01-23 19:21:59 - INFO - tridi.utils.training - Loaded optimizer from checkpoint
2026-01-23 19:21:59 - INFO - tridi.utils.training - Loaded scheduler from checkpoint
2026-01-23 19:21:59 - INFO - tridi.utils.training - Resumed state from checkpoint: step 15000, epoch 750
2026-01-23 19:21:59 - INFO - tridi.utils.training - Finished loading checkpoint
2026-01-23 19:21:59 - INFO - tridi.data.hh_dataset - HHDataset chi3d: loading from data/preprocessed/chi3d_smplx/dataset_train_25fps.hdf5.
2026-01-23 19:21:59 - INFO - tridi.data.hh_dataset - HH dataset chi3d train has 19768 frames (skipped_missing_seq=0).
2026-01-23 19:21:59 - INFO - tridi.data.hh_dataset - HHDataset chi3d: split=train #frames=19768
2026-01-23 19:21:59 - INFO - tridi.data.hh_dataset - HHDataset chi3d: loading from data/preprocessed/chi3d_smplx/dataset_val_25fps.hdf5.
2026-01-23 19:22:02 - INFO - tridi.data.hh_dataset - HH dataset chi3d val has 5548 frames (skipped_missing_seq=0).
2026-01-23 19:22:02 - INFO - tridi.data.hh_dataset - HHDataset chi3d: split=val #frames=5548
2026-01-23 19:22:02 - INFO - tridi.data - Train data length: 19768
2026-01-23 19:22:02 - INFO - tridi.data - Val data length: 5548
2026-01-23 19:22:05 - INFO - tridi.core.trainer - ***** Starting training *****
    Dataset train size: 19_768
    Dataset val size: 5_548
    Dataloader train size: 20
    Dataloader val size: 6
    Batch size per device = 1024
    Total train batch size (w. parallel, dist & accum) = 1024
    Gradient Accumulation steps = 1
    Max training steps = 300000
    Training state = TrainState(epoch=750, step=15000, initial_step=15000)
log_dir: /usr/prakt/w0012/HH_gen/experiments/001_chi3d_aug
cfg.resume.checkpoint: /usr/prakt/w0012/HH_gen/experiments/001_chi3d_aug/checkpoints/checkpoint-step-0015000.pth
